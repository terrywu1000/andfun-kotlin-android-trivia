{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xxacD8yR869p",
        "outputId": "658e52fd-e0a1-4fc9-c851-15ae1fb6a5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pymupdf>=1.24.10 (from pymupdf4llm)\n",
            "  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
            "Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, pymupdf4llm\n",
            "Successfully installed pymupdf-1.24.14 pymupdf4llm-0.0.17\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf4llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tAOkjl3869r"
      },
      "outputs": [],
      "source": [
        "# Use Case 1: Basic Markdown Extraction\n",
        "import pymupdf4llm\n",
        "\n",
        "# Extract PDF content as Markdown\n",
        "# md_text = pymupdf4llm.to_markdown(\"bengio03a.pdf\")\n",
        "# md_text = pymupdf4llm.to_markdown(\"/content/eqt_white.pdf\")\n",
        "md_text = pymupdf4llm.to_markdown(\"/content/数字政府一体化建设白皮书（2024年）.pdf\")\n",
        "\n",
        "\n",
        "print(md_text)  # Print first 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8DN1nfm869r"
      },
      "outputs": [],
      "source": [
        "# Use Case 2: Extracting specific pages\n",
        "import pymupdf4llm\n",
        "\n",
        "# Extract only pages 0 and 1 (first two pages)\n",
        "md_text = pymupdf4llm.to_markdown(\"/content/eqt_white.pdf\", pages=[9])\n",
        "print(md_text)  # Print first 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OEoQBJfD869r",
        "outputId": "b8117ed2-30ec-45c2-8539-664ccdb56706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/《中国新一代人工智能科技产业发展报告（2024）》.pdf...\n",
            "[                                        ] (0/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=                                       ] ( 1/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[===                                     ] ( 2/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=====                                   ] ( 3/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[======                                  ] ( 4/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[========                                ] ( 5/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[==========                              ] ( 6/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[===========                             ] ( 7/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=============                           ] ( 8/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[===============                         ] ( 9/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[================                        ] (10/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[==================                      ] (11/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[====================                    ] (12/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=====================                   ] (13/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=======================                 ] (14/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=========================               ] (15/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[==========================              ] (16/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[============================            ] (17/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[==============================          ] (18/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[===============================         ] (19/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[=================================       ] (20/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[===================================     ] (21/24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b=\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[====================================    ] (22/24)\b\b\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[======================================  ] (23/24)\b\b\b\b\b\b\b\b\b\b\b==\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[========================================] (24/24)\b\b\b\b\b\b\b\b\b]\n",
            "Markdown saved to output.md\n"
          ]
        }
      ],
      "source": [
        "# Use Case 3: Saving Markdown to a file\n",
        "import pymupdf4llm\n",
        "import pathlib\n",
        "\n",
        "md_text = pymupdf4llm.to_markdown(\"/content/《中国新一代人工智能科技产业发展报告（2024）》.pdf\")\n",
        "pathlib.Path(\"output.md\").write_bytes(md_text.encode())\n",
        "print(\"Markdown saved to output.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5um1lKT869s",
        "outputId": "7b9dd584-090f-4e11-e6f9-2eff389a212e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.18-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.18 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.15-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl.metadata (678 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai>=1.14.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.43.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.18->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (2024.6.0)\n",
            "Requirement already satisfied: httpx in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (10.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.4-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: pandas in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
            "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (4.4.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (0.14.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.18->llama-index) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.11.18-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.18-py3-none-any.whl (1.6 MB)\n",
            "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
            "   -------------------------- ------------- 1.0/1.6 MB 8.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.6/1.6 MB 7.0 MB/s eta 0:00:00\n",
            "Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.2/1.2 MB 15.1 MB/s eta 0:00:00\n",
            "Downloading llama_index_llms_openai-0.2.15-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.1.4-py3-none-any.whl (176 kB)\n",
            "Downloading llama_parse-0.5.10-py3-none-any.whl (12 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: striprtf, dirtyjson, nltk, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dirtyjson-1.0.8 llama-cloud-0.1.4 llama-index-0.11.18 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.18 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.15 llama-index-multi-modal-llms-openai-0.2.2 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.10 nltk-3.9.1 striprtf-0.0.26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "graphrag 0.1.1 requires lancedb<0.10.0,>=0.9.0, but you have lancedb 0.5.7 which is incompatible.\n",
            "graphrag 0.1.1 requires nltk==3.8.1, but you have nltk 3.9.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# required llama-index\n",
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PGVLjPe869s",
        "outputId": "b91f41be-39e9-4920-eb4e-5da1c17250ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n",
            "Number of LlamaIndex documents: 19\n",
            "Content of first document: ## A Neural Probabilistic Language Model\n",
            "\n",
            "**Yoshua Bengio** BENGIOY@IRO.UMONTREAL.CA\n",
            "**Réjean Ducharme** DUCHARME@IRO.UMONTREAL.CA\n",
            "**Pascal Vincent** VINCENTP@IRO.UMONTREAL.CA\n",
            "**Christian Jauvin** JAUVINC@IRO.UMONTREAL.CA\n",
            "_Département d’Informatique et Recherche Opérationnelle_\n",
            "_Centre de Recherche Mathématiques_\n",
            "_Université de Montréal, Montréal, Québec, Canada_\n",
            "\n",
            "**Editors: Jaz Kandola, Thomas Hofmann, Tomaso Poggio and John Shawe-Taylor**\n",
            "\n",
            "### Abstract\n",
            "\n",
            "A goal of statistical language modeling \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Use Case 4: Extracting as LlamaIndex document\n",
        "import pymupdf4llm\n",
        "\n",
        "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
        "llama_docs = llama_reader.load_data(\"bengio03a.pdf\")\n",
        "print(f\"Number of LlamaIndex documents: {len(llama_docs)}\")\n",
        "print(f\"Content of first document: {llama_docs[0].text[:500]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0HSje9m869s",
        "outputId": "955e58c1-b844-4a34-c888-2275c6096f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# Use Case 5: Image Extraction\n",
        "md_text_images = pymupdf4llm.to_markdown(doc=\"bengio03a.pdf\",\n",
        "                                         pages=[1, 11],\n",
        "                                         page_chunks=True,\n",
        "                                         write_images=True,\n",
        "                                         image_path=\"images\",\n",
        "                                         image_format=\"jpg\",\n",
        "                                         dpi=200)\n",
        "print(md_text_images[0]['images'])  # Print image information from the first chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1dwwopL869s",
        "outputId": "898716cf-4bee-4fa6-a502-409763dc9630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/3============[=============                           ] (1/============[==========================              ] (2/3=============[========================================] (3/3]\n",
            "{'metadata': {'format': 'PDF 1.1', 'title': 'bengio03a.dvi', 'author': '', 'subject': '', 'keywords': '', 'creator': 'dvips(k) 5.86 Copyright 1999 Radical Eye Software', 'producer': 'Acrobat Distiller Command 3.01 for Solaris 2.3 and later (SPARC)', 'creationDate': 'D:191030226135614', 'modDate': '', 'trapped': '', 'encryption': None, 'file_path': 'bengio03a.pdf', 'page_count': 19, 'page': 1}, 'toc_items': [], 'tables': [], 'images': [], 'graphics': [], 'text': '## A Neural Probabilistic Language Model\\n\\n**Yoshua Bengio** BENGIOY@IRO.UMONTREAL.CA\\n**Réjean Ducharme** DUCHARME@IRO.UMONTREAL.CA\\n**Pascal Vincent** VINCENTP@IRO.UMONTREAL.CA\\n**Christian Jauvin** JAUVINC@IRO.UMONTREAL.CA\\n_Département d’Informatique et Recherche Opérationnelle_\\n_Centre de Recherche Mathématiques_\\n_Université de Montréal, Montréal, Québec, Canada_\\n\\n**Editors: Jaz Kandola, Thomas Hofmann, Tomaso Poggio and John Shawe-Taylor**\\n\\n### Abstract\\n\\nA goal of statistical language modeling is to learn the joint probability function of sequences of\\nwords in a language. This is intrinsically difficult because of the curse of dimensionality: a word\\nsequence on which the model will be tested is likely to be different from all the word sequences seen\\nduring training. Traditional but very successful approaches based on n-grams obtain generalization\\nby concatenating very short overlapping sequences seen in the training set. We propose to fight the\\ncurse of dimensionality by learning a distributed representation for words which allows each\\ntraining sentence to inform the model about an exponential number of semantically neighboring\\nsentences. The model learns simultaneously (1) a distributed representation for each word along\\nwith (2) the probability function for word sequences, expressed in terms of these representations.\\nGeneralization is obtained because a sequence of words that has never been seen before gets high\\nprobability if it is made of words that are similar (in the sense of having a nearby representation) to\\nwords forming an already seen sentence. Training such large models (with millions of parameters)\\nwithin a reasonable time is itself a significant challenge. We report on experiments using neural\\nnetworks for the probability function, showing on two text corpora that the proposed approach\\nsignificantly improves on state-of-the-art n-gram models, and that the proposed approach allows to\\ntake advantage of longer contexts.\\n\\n**Keywords:** Statistical language modeling, artificial neural networks, distributed representation,\\ncurse of dimensionality\\n\\n### 1. Introduction\\n\\nA fundamental problem that makes language modeling and other learning problems difficult is the\\n_curse of dimensionality. It is particularly obvious in the case when one wants to model the joint_\\ndistribution between many discrete random variables (such as words in a sentence, or discrete attributes in a data-mining task). For example, if one wants to model the joint distribution of 10\\nconsecutive words in a natural language with a vocabulary V of size 100,000, there are potentially\\n100000[10] 1 = 10[50] 1 free parameters. When modeling continuous variables, we obtain gen_−_ _−_\\neralization more easily (e.g. with smooth classes of functions like multi-layer neural networks or\\nGaussian mixture models) because the function to be learned can be expected to have some local smoothness properties. For discrete spaces, the generalization structure is not as obvious: any\\nchange of these discrete variables may have a drastic impact on the value of the function to be esti\\n_⃝c_ 2003 Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin.\\n\\n\\n-----\\n\\n', 'words': []}\n"
          ]
        }
      ],
      "source": [
        "# Use Case 6: Chunking with Metadata\n",
        "md_text_chunks = pymupdf4llm.to_markdown(doc=\"bengio03a.pdf\",\n",
        "                                         pages=[0, 1, 2],\n",
        "                                         page_chunks=True)\n",
        "print(md_text_chunks[0])  # Print the first chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1PGib7F869s",
        "outputId": "74492652-e83f-46ef-819a-ad6bba5fe996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing bengio03a.pdf...\n",
            "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
            "[(90.0, 93.28394317626953, 119.24702453613281, 104.19293975830078, 'mated,', 0, 0, 0), (122.51972961425781, 93.28394317626953, 138.18504333496094, 104.19293975830078, 'and', 0, 0, 1), (141.37046813964844, 93.28394317626953, 164.95570373535156, 104.19293975830078, 'when', 0, 0, 2), (168.14112854003906, 93.28394317626953, 181.3955535888672, 104.19293975830078, 'the', 0, 0, 3), (184.59188842773438, 93.28394317626953, 217.7770538330078, 104.19293975830078, 'number', 0, 0, 4)]\n"
          ]
        }
      ],
      "source": [
        "# Use Case 7: Word Extraction\n",
        "md_text_words = pymupdf4llm.to_markdown(doc=\"bengio03a.pdf\",\n",
        "                                        pages=[1,2],\n",
        "                                        page_chunks=True,\n",
        "                                        write_images=True,\n",
        "                                        image_path=\"images\",\n",
        "                                        image_format=\"jpg\",\n",
        "                                        dpi=200,\n",
        "                                        extract_words=True)\n",
        "print(md_text_words[0]['words'][:5])  # Print the first 5 words from the first chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7136ih5u869s",
        "outputId": "cf553948-7c1d-4c05-afa2-42a9d13a0d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 2408.14717v1.pdf...\n",
            "[                                        ] (0/1=======================================[========================================] (1/1]\n"
          ]
        }
      ],
      "source": [
        "# Use Case 5: Table Extraction\n",
        "import pymupdf4llm\n",
        "import json\n",
        "\n",
        "md_text_tables = pymupdf4llm.to_markdown(doc=\"2408.14717v1.pdf\",\n",
        "                                         pages=[4],  # Specify pages containing tables\n",
        "                                         )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxQoixqB869s",
        "outputId": "d28cc4e9-f7af-4a0e-b928-00cd0ab602fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text2SQL is Not Enough: Unifying AI and Databases with TAG\n",
            "\n",
            "**Table 1: Accuracy and execution time (ET) for TAG benchmark queries, averaged over all queries and each query type: TAG**\n",
            "**significantly improves answer quality while achieving the fastest or nearly fastest execution time.**\n",
            "\n",
            "Overall Match-based Comparison Ranking Aggregation\n",
            "Method\n",
            "\n",
            "Exact Match ↑[1] ET (s) ↓ Exact Match ↑ ET (s) ↓ Exact Match ↑ ET (s) ↓ Exact Match ↑ ET (s) Exact Match ↑ ET (s) ↓\n",
            "\n",
            "Text2SQL 0.17 5.63 0.20 4.72 0.20 4.01 0.10 7.26 N/A 6.53\n",
            "RAG 0.00 3.23 0.00 3.73 0.00 **2.29** 0.00 **2.01** N/A 4.89\n",
            "Retrieval + LM Rank 0.02 4.82 0.00 6.20 0.05 4.19 0.00 3.42 N/A 5.46\n",
            "Text2SQL + LM 0.13 9.08 0.10 11.25 0.10 3.89 0.20 11.80 N/A 9.38\n",
            "Hand-written TAG **0.55** **2.94** **0.60** **1.70** **0.65** 5.05 **0.40** 2.50 N/A **2.50**\n",
            "\n",
            "1 Excludes aggregation since accuracy is not measured.\n",
            "\n",
            "#### Query: Provide information about the races held on Sepang International Circuit\n",
            "\n",
            " RAG Text2SQL + LM Hand-written TAG\n",
            "\n",
            "Based on the given data points, the following information The data points provided do not contain specific The Sepang International Circuit in Kuala Lumpur,\n",
            "is available about the races held on Sepang International information about the races held on Sepang International Malaysia, hosted the Malaysian Grand Prix from 1999 to\n",
            "Circuit: The races held on this circuit are the Malaysian Circuit. However, based on general knowledge, the 2017. The races were held on the following dates: 1999:\n",
            "Grand Prix. The location of the circuit is Kuala Lumpur, Sepang International Circuit is a racing circuit in Sepang, October 17 (15th round) 2000: October 22 (2nd round),\n",
            "Malaysia. The following races were held on this circuit: Malaysia, and it has hosted the Malaysian Grand Prix, a 2001: March 18 (2nd round), 2002: March 17 (2nd\n",
            "2001 Malaysian Grand Prix, 2003 Malaysian Grand Prix, Formula One World Championship event, from 1999 to round), 2003: March 23 (2nd round), 2004: March 21\n",
            "2002 Malaysian Grand Prix, 2000 Malaysian Grand Prix. 2017. (2nd round), 2005: March 20 (2nd round), …, 2016:\n",
            "\n",
            "October 2 (16th round), 2017: October 1 (15th round).\n",
            "\n",
            "**Figure 2: Example Aggregation Results: The RAG baseline provides an incomplete answer to the query while Text2SQL + LM**\n",
            "**fails to answer the question using any data from the DB. The Hand-written TAG baseline provides the most thorough answer,**\n",
            "**synthesizing data from the DB and its own world knowledge.**\n",
            "\n",
            "|Overall Match-based Comparison Ranking Aggregation Method Exact Match ↑1 ET (s) ↓ Exact Match ↑ ET (s) ↓ Exact Match ↑ ET (s) ↓ Exact Match ↑ ET (s) Exact Match ↑ ET (s) ↓|Col2|Col3|Col4|Col5|Col6|\n",
            "|---|---|---|---|---|---|\n",
            "|Text2SQL RAG Retrieval + LM Rank Text2SQL + LM Hand-written TAG|0.17 5.63 0.00 3.23 0.02 4.82 0.13 9.08 0.55 2.94|0.20 4.72 0.00 3.73 0.00 6.20 0.10 11.25 0.60 1.70|0.20 4.01 0.00 2.29 0.05 4.19 0.10 3.89 0.65 5.05|0.10 7.26 0.00 2.01 0.00 3.42 0.20 11.80 0.40 2.50|N/A 6.53 N/A 4.89 N/A 5.46 N/A 9.38 N/A 2.50|\n",
            "\n",
            "|Query: Provide information about the races held on Sepang International Circuit|Col2|Col3|\n",
            "|---|---|---|\n",
            "|RAG|Text2SQL + LM|Hand-written TAG|\n",
            "|Based on the given data points, the following information is available about the races held on Sepang International Circuit: The races held on this circuit are the Malaysian Grand Prix. The location of the circuit is Kuala Lumpur, Malaysia. The following races were held on this circuit: 2001 Malaysian Grand Prix, 2003 Malaysian Grand Prix, 2002 Malaysian Grand Prix, 2000 Malaysian Grand Prix.|The data points provided do not contain specific information about the races held on Sepang International Circuit. However, based on general knowledge, the Sepang International Circuit is a racing circuit in Sepang, Malaysia, and it has hosted the Malaysian Grand Prix, a Formula One World Championship event, from 1999 to 2017.|The Sepang International Circuit in Kuala Lumpur, Malaysia, hosted the Malaysian Grand Prix from 1999 to 2017. The races were held on the following dates: 1999: October 17 (15th round) 2000: October 22 (2nd round), 2001: March 18 (2nd round), 2002: March 17 (2nd round), 2003: March 23 (2nd round), 2004: March 21 (2nd round), 2005: March 20 (2nd round), …, 2016: October 2 (16th round), 2017: October 1 (15th round).|\n",
            "\n",
            "\n",
            "**Table 2: TAG benchmark results averaged over queries re-**\n",
            "**quiring Knowledge or Reasoning: TAG performs consistently**\n",
            "**well with above 50% exact match accuracy on both Knowl-**\n",
            "**edge and Reasoning query types.**\n",
            "\n",
            "Knowledge Reasoning\n",
            "Method\n",
            "\n",
            "Exact Match ↑ ET (s) ↓ Exact Match ↑ ET (s) ↓\n",
            "\n",
            "Text2SQL 0.20 5.23 0.10 5.52\n",
            "RAG 0.00 **2.73** 0.00 2.58\n",
            "Retrieval + LM Rank 0.03 4.97 0.00 3.87\n",
            "Text2SQL + LM 0.10 10.27 0.20 6.39\n",
            "Hand-written TAG **0.53** 3.50 **0.60** **2.24**\n",
            "\n",
            "#### 5 RELATED WORK\n",
            "\n",
            "**_Text2SQL Text2SQL using LMs has been extensively explored in_**\n",
            "prior work. WikiSQL [33], Spider [29], and BIRD [17] are all popular datasets for cross-domain Text2SQL. These datasets contain\n",
            "structured data across many domains on which the task of converting natural language queries to SQL is evaluated. However, this\n",
            "direction does not utilize model capabilities beyond SQL generation, keeping queries that require reasoning or knowledge beyond\n",
            "a static data source out of scope.\n",
            "\n",
            "**_Retrieval Augmented Generation Retrieval augmented gener-_**\n",
            "ation (RAG) [16] enables LMs to extend beyond their parametric\n",
            "\n",
            "\n",
            "knowledge to large collections of text. SQuAD [22] and HotPotQA\n",
            "\n",
            "[27] focus on question-answering over single document and multiple document sources respectively. The dense table retrieval (DTR)\n",
            "model [11] extends RAG to tabular data, embedding tabular context\n",
            "to retrieve relevant cells and rows for a query. Join-aware table retrieval [6] adds a table-table similarity score term to the DTR model\n",
            "to improve performance on complex queries involving joined tables. In contrast to prior RAG work, the TAG model encompasses\n",
            "a larger field of queries users have over their data by leveraging\n",
            "LM capabilities in the query execution step and allowing DBMS\n",
            "operations for exact computation over large amounts of data.\n",
            "\n",
            "**_NL Queries over Semi-structured Data Prior work has explored_**\n",
            "the relational information between table entities and unstructured\n",
            "entity fields in semi-structured data sources. STaRK [25] evaluates\n",
            "table retrieval methodologies across semi-structured knowledge\n",
            "bases (SKBs), including both structural and nonstructural information. SUQL [20] addresses the task of conversational search, where\n",
            "an LM is used as a semantic parser to handle unstructured components user queries over hybrid data. While these works primarily\n",
            "focus on natural language search queries over semi-structured data,\n",
            "we seek to explore a broader range of queries leveraging more LM\n",
            "capabilities for tasks beyond search and lookup.\n",
            "\n",
            "**_Agentic Data Assistants Recent work has explored LM agents_**\n",
            "as data assistants [12]. Spider2-V [4] explores multimodal agent\n",
            "\n",
            "|edge and Reasoning query types.|Col2|Col3|\n",
            "|---|---|---|\n",
            "|Knowledge Reasoning Method Exact Match ↑ ET (s) ↓ Exact Match ↑ ET (s) ↓|||\n",
            "|Text2SQL RAG Retrieval + LM Rank Text2SQL + LM Hand-written TAG|0.20 5.23 0.00 2.73 0.03 4.97 0.10 10.27 0.53 3.50|0.10 5.52 0.00 2.58 0.00 3.87 0.20 6.39 0.60 2.24|\n",
            "\n",
            "\n",
            "-----\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(md_text_tables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vl7XtL1869t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}